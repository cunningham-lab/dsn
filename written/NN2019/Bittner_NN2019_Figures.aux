\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Figures}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip A. \textit  {Neural data analysis}: To identify structure in experimentally recorded neural data sets (left), neural data analysts use the modern inference engine (center) -- a collection of tools like convex optimization theory and deep learning from the machine learning literature -- to carefully execute Bayesian inference on a phenomenological generative model of the data. Neural data analysts impose practical constraints to these phenomenological models so that Bayesian inference can be done. \textit  {Theoretical Neuroscience}: Theoretical neuroscientists design systems of equations reflecting biological reality comprising a model (right, e.g. the STG) with the hopes of reproducing emergent properties of neural computation found in data (left). In standard practice, models are evaluated by simulating them for many choices of parameters on a server (center). To gain an understanding for how parameters of the model govern such emergent properties, theoretical neuroscientists are left to measure correlations or find other structure in the simulated activity. In this study, we introduce DSNs, which use the modern inference engine to learn distributions of model solutions given emergent properties of neural computations (yellow boxes). B. We use DSNs to provide novel insights to popular models in theoretical neuroscience. We examine network syncing in the STG, blah in a dynamic four neuron-type population model of primary visual cortex (V1), information routing in the superior colliculus (SC), and blah in low-rank recurrent neural networks (RNNs).}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A. Degenerate solution networks (DSNs) are deep probability distributions $q_\theta (z)$ of theoretical model parameterizations that produce emergent properties of interest. The stochasticity of a deep probability distribution comes from a simple random variable $\omega \sim q_0$, where $q_0$ is often chosen to be an isotropic gaussian, and the structure comes from the deterministic transformation made by the deep neural network with optimized parameters $\theta $. DSNs are the result of a constrained stochastic optimization, in which emergent properties $T(x)$ are fixed in expectation over model simulations $x \sim p(x \mid z)$ and DSN samples $z \sim q_\theta (z)$ to be a particular value $\mu $. DSNs distributions maximize randomness through entropy. B. For a choice of model (STG) and emergent property (network syncing), a DSN learns a posterior distribution of the model parameters $z = \left [g_{\text  {el}}, g_{\text  {synA}} \right ]^\top $ conditioned on network syncing.}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \input {models/V1/V1_caption.tex}}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \input {models/SC/SC_caption.tex}}}{7}}
